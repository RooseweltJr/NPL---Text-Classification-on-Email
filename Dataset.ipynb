{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.data.path.append(r'C:\\Users\\roose\\AppData\\Roaming\\nltk_data')\n",
    "\n",
    "# # Baixar os pacotes necess치rios (se necess치rio)\n",
    "nltk.download('punkt', download_dir=r'C:\\Users\\roose\\AppData\\Roaming\\nltk_data')\n",
    "nltk.download('stopwords', download_dir=r'C:\\Users\\roose\\AppData\\Roaming\\nltk_data')\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "caminho_atual = os.getcwd()\n",
    "\n",
    "base = os.path.join(caminho_atual, 'Data')\n",
    "\n",
    "folders = os.listdir(base)\n",
    "data = []\n",
    "\n",
    "# Ler os arquivos e juntar em um arquivo s칩\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(base, folder))\n",
    "    for file in files:\n",
    "        try:\n",
    "            if file == 'combine.csv':\n",
    "                pass\n",
    "            with open(os.path.join(base, folder, file), encoding='utf-8') as f:\n",
    "                contents = \" \".join(f.readlines())\n",
    "                data.append([file.split(\".\")[0], folder, contents])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "df = pd.DataFrame(data, columns=['ID', 'Categoria', 'Conteudo'])\n",
    "\n",
    "df\n",
    "df['Categoria'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = list(df.Conteudo.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = list(df.Conteudo.unique())\n",
    "dic = dict(df)\n",
    "\n",
    "uni = {}\n",
    "i = 0\n",
    "for k in range(len(list(dic['Conteudo']))):\n",
    "    if dic['Conteudo'][k] in unique:\n",
    "        uni[i] = [dic['Conteudo'][k], dic['Categoria'][k],dic['ID'][k]]\n",
    "        unique.remove(dic['Conteudo'][k])\n",
    "        i += 1\n",
    "\n",
    "\n",
    "data = pd.DataFrame(uni).T\n",
    "print(data.shape)\n",
    "data.columns = ['Conteudo', 'Categoria','Id']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wordcloud(words,title):\n",
    "    cloud = WordCloud(width=1920, height=1080,max_font_size=200, max_words=300, background_color=\"white\").generate(words)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(cloud, interpolation=\"gaussian\")\n",
    "    plt.axis(\"off\") \n",
    "    plt.title(title, fontsize=60)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "for punct in punctuation:\n",
    "    stop.append(punct)\n",
    "\n",
    "def filter_text(text, stop_words):\n",
    "    word_tokens = WordPunctTokenizer().tokenize(text.lower())\n",
    "    filtered_text = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha() and len(w) > 3]\n",
    "    filtered_text = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in filtered_text if not w in stop_words] \n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"filtered_text\"] = data.Conteudo.apply(lambda x : filter_text(x, stop)) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(data[data.Categoria == \"Crime\"].filtered_text) \n",
    "make_wordcloud(all_text, \"Crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = pd.DataFrame(all_text.split(), columns = ['words'])\n",
    "top_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x = top_10.words.value_counts().index,\n",
    "            y = top_10.words.value_counts(), palette = sns.color_palette(\"mako\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora o seu c칩digo deve funcionar corretamente\n",
    "# Configurar as stopwords e o lemmatizer\n",
    "en_stopwords = stopwords.words('english')\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_contents_w_stopwords(contents):\n",
    "    tokenized_word = word_tokenize(str(contents).replace(\"\\n\", \"\").replace(\"\\\\\", \"\").replace(\">\", \"\").strip())\n",
    "    tokenized_word_w_stopwords = []\n",
    "    for word in tokenized_word:\n",
    "        word = regex.sub(u'\\p{^Latin}', u'', word.lower())\n",
    "        if len(word) > 3 and word.strip().lower() not in en_stopwords:\n",
    "            word = word_lemmatizer.lemmatize(word)\n",
    "            tokenized_word_w_stopwords.append(word)\n",
    "    tokenized_word_w_stopwords = \" \".join(tokenized_word_w_stopwords)\n",
    "    return tokenized_word_w_stopwords\n",
    "\n",
    "# Exemplo de uso\n",
    "comment_words = df[df['ID'] == 14991]['Conteudo'].to_string().replace(\"\\n\", \"\").replace(\"\\\\\", \"\").replace(\">\", \"\").strip()\n",
    "print(comment_words)\n",
    "\n",
    "tokenized_word_w_stopwords = clean_contents_w_stopwords(comment_words)\n",
    "print(tokenized_word_w_stopwords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
